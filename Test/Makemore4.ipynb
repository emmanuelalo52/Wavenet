{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt','r').read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 8]) torch.Size([182625])\n",
      "torch.Size([22655, 8]) torch.Size([22655])\n",
      "torch.Size([22866, 8]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "# build the dataset\n",
    "block_size = 8 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ --> y\n",
      ".......y --> u\n",
      "......yu --> h\n",
      ".....yuh --> e\n",
      "....yuhe --> n\n",
      "...yuhen --> g\n",
      "..yuheng --> .\n",
      "........ --> d\n",
      ".......d --> i\n",
      "......di --> o\n",
      ".....dio --> n\n",
      "....dion --> d\n",
      "...diond --> r\n",
      "..diondr --> e\n",
      ".diondre --> .\n",
      "........ --> x\n",
      ".......x --> a\n",
      "......xa --> v\n",
      ".....xav --> i\n",
      "....xavi --> e\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip(Xtr[:20], Ytr[:20]):\n",
    "  print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create linear\n",
    "class Linear:\n",
    "    def __init__(self,fan_in,fan_out,bias=True):\n",
    "        self.weight = torch.randn(fan_in, fan_out)/ fan_out * 0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "    def __call__(self,x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "    \n",
    "\n",
    "#Create BachNorm1D\n",
    "class BatchNorm1D:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      if x.ndim == 2:\n",
    "        dim = 0\n",
    "      elif x.ndim == 3:\n",
    "        dim = (0,1)\n",
    "      xmean = x.mean(dim, keepdim=True) # batch mean\n",
    "      xvar = x.var(dim, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return [self.beta,self.gamma] # for backpropagation\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "  \n",
    "\n",
    "class FlattenConsecutive:\n",
    "    def __init__(self,n):\n",
    "        self.n = n\n",
    "    def __call__(self,x):\n",
    "        B,T,C = x.shape\n",
    "        x = x.view(B,T//self.n,C*self.n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class Embedding:\n",
    "  \n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    \n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX]\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "    \n",
    "\n",
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # get parameters of all layers and stretch them out into one list\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def __call__(self,x):\n",
    "        x = torch.max(x, dim=-1, keepdim=True)\n",
    "        z = torch.exp(x)\n",
    "        sum_z = z.sum()\n",
    "        self.out = z/sum_z\n",
    "        return self.out\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class GLU:\n",
    "    def __init__(self,chan_in,chan_out,bias=True):\n",
    "        self.weight = torch.randn(chan_in,chan_out)/(chan_in)*0.5\n",
    "        self.bias = torch.zeros(chan_out) if bias else None\n",
    "    def __call__(self,x):\n",
    "        assert x.ndim >=2, \"Input tensor must have at least 2 dimensions\"\n",
    "        linear = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            linear += self.bias\n",
    "        linear_out, gate = torch.chunk(linear, 2, dim=-1)\n",
    "        \n",
    "        gate = torch.sigmoid(gate)\n",
    "        self.out = torch.tanh(linear_out) * gate\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42); #set rng for reproductivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76579\n"
     ]
    }
   ],
   "source": [
    "n_emb = 24 #dimensionality of the character embedding vectors\n",
    "n_hidden = 128 #Number of neurons in the hidden layer\n",
    "# C = torch.randn((vocab_size,n_emb))\n",
    "# model = Sequential([\n",
    "#   Embedding(vocab_size, n_emb),\n",
    "#   FlattenConsecutive(2), Linear(n_emb * 2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "#   FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "#   FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "#   Linear(n_hidden, vocab_size),\n",
    "# ])\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "   Embedding(vocab_size, n_emb),\n",
    "   FlattenConsecutive(2), GLU(n_emb * 2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "   FlattenConsecutive(2), GLU(n_hidden*2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "   FlattenConsecutive(2), GLU(n_hidden*2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "   GLU(n_hidden, vocab_size),\n",
    " ])\n",
    "\n",
    "with torch.no_grad():\n",
    "  model.layers[-1].weight *= 0.1\n",
    "\n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2963\n",
      "   1000/ 200000: 2.1594\n",
      "   2000/ 200000: 2.4213\n",
      "   3000/ 200000: 2.0733\n",
      "   4000/ 200000: 2.3041\n",
      "   5000/ 200000: 2.1589\n",
      "   6000/ 200000: 2.1546\n",
      "   7000/ 200000: 2.1296\n",
      "   8000/ 200000: 2.4750\n",
      "   9000/ 200000: 2.0119\n",
      "  10000/ 200000: 2.0702\n",
      "  11000/ 200000: 2.2716\n",
      "  12000/ 200000: 1.9945\n",
      "  13000/ 200000: 2.4214\n",
      "  14000/ 200000: 1.9221\n",
      "  15000/ 200000: 1.9830\n",
      "  16000/ 200000: 1.9716\n",
      "  17000/ 200000: 2.1320\n",
      "  18000/ 200000: 2.2565\n",
      "  19000/ 200000: 2.2727\n",
      "  20000/ 200000: 1.9151\n",
      "  21000/ 200000: 2.3956\n",
      "  22000/ 200000: 2.5075\n",
      "  23000/ 200000: 1.7348\n",
      "  24000/ 200000: 2.1403\n",
      "  25000/ 200000: 2.1294\n",
      "  26000/ 200000: 2.1008\n",
      "  27000/ 200000: 2.2249\n",
      "  28000/ 200000: 1.9280\n",
      "  29000/ 200000: 2.3139\n",
      "  30000/ 200000: 2.1184\n",
      "  31000/ 200000: 2.2699\n",
      "  32000/ 200000: 2.4948\n",
      "  33000/ 200000: 1.9422\n",
      "  34000/ 200000: 1.7589\n",
      "  35000/ 200000: 1.7637\n",
      "  36000/ 200000: 2.0716\n",
      "  37000/ 200000: 2.1046\n",
      "  38000/ 200000: 1.9806\n",
      "  39000/ 200000: 1.4946\n",
      "  40000/ 200000: 2.1039\n",
      "  41000/ 200000: 2.0890\n",
      "  42000/ 200000: 1.9928\n",
      "  43000/ 200000: 1.9401\n",
      "  44000/ 200000: 1.7724\n",
      "  45000/ 200000: 2.2301\n",
      "  46000/ 200000: 1.6595\n",
      "  47000/ 200000: 1.7034\n",
      "  48000/ 200000: 2.3342\n",
      "  49000/ 200000: 2.1898\n",
      "  50000/ 200000: 1.9853\n",
      "  51000/ 200000: 1.7020\n",
      "  52000/ 200000: 1.7728\n",
      "  53000/ 200000: 2.0359\n",
      "  54000/ 200000: 2.2200\n",
      "  55000/ 200000: 1.9573\n",
      "  56000/ 200000: 2.1022\n",
      "  57000/ 200000: 1.8885\n",
      "  58000/ 200000: 2.1232\n",
      "  59000/ 200000: 2.0321\n",
      "  60000/ 200000: 1.9989\n",
      "  61000/ 200000: 2.1693\n",
      "  62000/ 200000: 2.3083\n",
      "  63000/ 200000: 1.9596\n",
      "  64000/ 200000: 2.5020\n",
      "  65000/ 200000: 2.1160\n",
      "  66000/ 200000: 1.9483\n",
      "  67000/ 200000: 1.9117\n",
      "  68000/ 200000: 1.6728\n",
      "  69000/ 200000: 1.3453\n",
      "  70000/ 200000: 2.0724\n",
      "  71000/ 200000: 2.0321\n",
      "  72000/ 200000: 1.6217\n",
      "  73000/ 200000: 1.6539\n",
      "  74000/ 200000: 1.8521\n",
      "  75000/ 200000: 1.4824\n",
      "  76000/ 200000: 1.6895\n",
      "  77000/ 200000: 1.9840\n",
      "  78000/ 200000: 2.8842\n",
      "  79000/ 200000: 1.9366\n",
      "  80000/ 200000: 2.5803\n",
      "  81000/ 200000: 1.8217\n",
      "  82000/ 200000: 1.7207\n",
      "  83000/ 200000: 1.7500\n",
      "  84000/ 200000: 2.0021\n",
      "  85000/ 200000: 1.7122\n",
      "  86000/ 200000: 1.8404\n",
      "  87000/ 200000: 2.6465\n",
      "  88000/ 200000: 1.7270\n",
      "  89000/ 200000: 2.0175\n",
      "  90000/ 200000: 1.5156\n",
      "  91000/ 200000: 1.6141\n",
      "  92000/ 200000: 1.9768\n",
      "  93000/ 200000: 2.3051\n",
      "  94000/ 200000: 1.8697\n",
      "  95000/ 200000: 2.0976\n",
      "  96000/ 200000: 2.2282\n",
      "  97000/ 200000: 1.8392\n",
      "  98000/ 200000: 1.9144\n",
      "  99000/ 200000: 1.5454\n",
      " 100000/ 200000: 1.9501\n",
      " 101000/ 200000: 2.0501\n",
      " 102000/ 200000: 2.0887\n",
      " 103000/ 200000: 1.9817\n",
      " 104000/ 200000: 1.6412\n",
      " 105000/ 200000: 2.0310\n",
      " 106000/ 200000: 2.0041\n",
      " 107000/ 200000: 2.0768\n",
      " 108000/ 200000: 2.0012\n",
      " 109000/ 200000: 2.0176\n",
      " 110000/ 200000: 2.0699\n",
      " 111000/ 200000: 2.1336\n",
      " 112000/ 200000: 1.6998\n",
      " 113000/ 200000: 1.9970\n",
      " 114000/ 200000: 2.3399\n",
      " 115000/ 200000: 1.9629\n",
      " 116000/ 200000: 2.1836\n",
      " 117000/ 200000: 1.8975\n",
      " 118000/ 200000: 2.1010\n",
      " 119000/ 200000: 2.1894\n",
      " 120000/ 200000: 1.4751\n",
      " 121000/ 200000: 2.1317\n",
      " 122000/ 200000: 1.9725\n",
      " 123000/ 200000: 1.3643\n",
      " 124000/ 200000: 1.5688\n",
      " 125000/ 200000: 2.0171\n",
      " 126000/ 200000: 1.6320\n",
      " 127000/ 200000: 1.9093\n",
      " 128000/ 200000: 2.4473\n",
      " 129000/ 200000: 1.8527\n",
      " 130000/ 200000: 1.5947\n",
      " 131000/ 200000: 1.8203\n",
      " 132000/ 200000: 1.8805\n",
      " 133000/ 200000: 1.7184\n",
      " 134000/ 200000: 2.3312\n",
      " 135000/ 200000: 1.8829\n",
      " 136000/ 200000: 1.9080\n",
      " 137000/ 200000: 2.2674\n",
      " 138000/ 200000: 1.8383\n",
      " 139000/ 200000: 1.6335\n",
      " 140000/ 200000: 1.8239\n",
      " 141000/ 200000: 1.7294\n",
      " 142000/ 200000: 1.5841\n",
      " 143000/ 200000: 1.7439\n",
      " 144000/ 200000: 1.9093\n",
      " 145000/ 200000: 2.1994\n",
      " 146000/ 200000: 1.7788\n",
      " 147000/ 200000: 2.3054\n",
      " 148000/ 200000: 1.7512\n",
      " 149000/ 200000: 1.6231\n",
      " 150000/ 200000: 1.6801\n",
      " 151000/ 200000: 1.8957\n",
      " 152000/ 200000: 1.8037\n",
      " 153000/ 200000: 2.0731\n",
      " 154000/ 200000: 2.4047\n",
      " 155000/ 200000: 2.3523\n",
      " 156000/ 200000: 1.8840\n",
      " 157000/ 200000: 2.1267\n",
      " 158000/ 200000: 1.7602\n",
      " 159000/ 200000: 1.8539\n",
      " 160000/ 200000: 2.1923\n",
      " 161000/ 200000: 1.9889\n",
      " 162000/ 200000: 1.5830\n",
      " 163000/ 200000: 1.4277\n",
      " 164000/ 200000: 1.9049\n",
      " 165000/ 200000: 1.5045\n",
      " 166000/ 200000: 1.7167\n",
      " 167000/ 200000: 1.7139\n",
      " 168000/ 200000: 1.9485\n",
      " 169000/ 200000: 1.9871\n",
      " 170000/ 200000: 1.6862\n",
      " 171000/ 200000: 2.2287\n",
      " 172000/ 200000: 1.7367\n",
      " 173000/ 200000: 1.9365\n",
      " 174000/ 200000: 1.6694\n",
      " 175000/ 200000: 1.6007\n",
      " 176000/ 200000: 1.9705\n",
      " 177000/ 200000: 2.0150\n",
      " 178000/ 200000: 2.1171\n",
      " 179000/ 200000: 1.5884\n",
      " 180000/ 200000: 1.9512\n",
      " 181000/ 200000: 1.7485\n",
      " 182000/ 200000: 2.3464\n",
      " 183000/ 200000: 2.1108\n",
      " 184000/ 200000: 1.8255\n",
      " 185000/ 200000: 2.3599\n",
      " 186000/ 200000: 1.6934\n",
      " 187000/ 200000: 2.2543\n",
      " 188000/ 200000: 1.8229\n",
      " 189000/ 200000: 1.8304\n",
      " 190000/ 200000: 1.5030\n",
      " 191000/ 200000: 2.1754\n",
      " 192000/ 200000: 2.1600\n",
      " 193000/ 200000: 1.7337\n",
      " 194000/ 200000: 1.5734\n",
      " 195000/ 200000: 1.6041\n",
      " 196000/ 200000: 1.6211\n",
      " 197000/ 200000: 1.5208\n",
      " 198000/ 200000: 1.5790\n",
      " 199000/ 200000: 1.9859\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    #mini batch\n",
    "    ix = torch.randint(0,Xtr.shape[0],(batch_size,)) #batch size\n",
    "    Xb,Yb = Xtr[ix],Ytr[ix]\n",
    "    #forward pass\n",
    "    # emb = C[Xb] #embed the characters into vectors\n",
    "    # x = emb.view(emb.shape[0],-1) #concatenate the vectors\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits,Yb)\n",
    "    #back pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    # lr = lrs[i]\n",
    "    lr = 0.1 if 1 < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    #track stats\n",
    "    if i % 1000 == 0:\n",
    "        print(f'{i:7d}/{max_steps: 7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layers in eval mode\n",
    "for layer in model.layers:\n",
    "  layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1.8405946493148804\n",
      "val 2.0387485027313232\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  logits = model(x)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marcitia.\n",
      "isidh.\n",
      "krishik.\n",
      "jima.\n",
      "kaizon.\n",
      "clarce.\n",
      "dusti.\n",
      "mckenna.\n",
      "eide.\n",
      "keylse.\n",
      "khaley.\n",
      "samari.\n",
      "emelia.\n",
      "drun.\n",
      "erisha.\n",
      "gwellton.\n",
      "kolvin.\n",
      "raelann.\n",
      "jhonestobis.\n",
      "maryhon.\n"
     ]
    }
   ],
   "source": [
    "#model test\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass\n",
    "      logits = model(torch.tensor([context]))\n",
    "      # sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
