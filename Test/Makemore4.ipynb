{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt','r').read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 8]) torch.Size([182625])\n",
      "torch.Size([22655, 8]) torch.Size([22655])\n",
      "torch.Size([22866, 8]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "# build the dataset\n",
    "block_size = 8 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ --> y\n",
      ".......y --> u\n",
      "......yu --> h\n",
      ".....yuh --> e\n",
      "....yuhe --> n\n",
      "...yuhen --> g\n",
      "..yuheng --> .\n",
      "........ --> d\n",
      ".......d --> i\n",
      "......di --> o\n",
      ".....dio --> n\n",
      "....dion --> d\n",
      "...diond --> r\n",
      "..diondr --> e\n",
      ".diondre --> .\n",
      "........ --> x\n",
      ".......x --> a\n",
      "......xa --> v\n",
      ".....xav --> i\n",
      "....xavi --> e\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip(Xtr[:20], Ytr[:20]):\n",
    "  print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create linear\n",
    "class Linear:\n",
    "    def __init__(self,fan_in,fan_out,bias=True):\n",
    "        self.weight = torch.randn(fan_in, fan_out)/ fan_out * 0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "    def __call__(self,x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "    \n",
    "\n",
    "#Create BachNorm1D\n",
    "class BatchNorm1D:\n",
    "    def __init__(self, dim,eps= 1e-5,momentum=0.1):\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        self.training = True\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "    def __call__(self,x):\n",
    "        if self.training:\n",
    "            if x.ndim == 2:\n",
    "                dim = 0\n",
    "            elif x.ndim == 3:\n",
    "                dim = (0,1)\n",
    "            xmean = x.mean(0,keepdim=True)\n",
    "            xvar = x.var(0,keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        #buffer\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1-self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1-self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.beta,self.gamma] # for backpropagation\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "  \n",
    "\n",
    "class FlattenConsecutive:\n",
    "    def __init__(self,n):\n",
    "        self.n = n\n",
    "    def __call__(self,x):\n",
    "        B,T,C = x.shape\n",
    "        x = x.view(B,T//self.n,C*self.n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class Embedding:\n",
    "  \n",
    "    def __init__(self, num_embd, embd_dim):\n",
    "        self.weight = torch.randn((num_embd, embd_dim))\n",
    "    \n",
    "    def __call__(self, IX):\n",
    "        self.out = self.weight[IX]\n",
    "        return self.out\n",
    "  \n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "    \n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "             x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters() ]\n",
    "                \n",
    "\n",
    "\n",
    "class GLU:\n",
    "    def __init__(self,chan_in,chan_out,bias=True):\n",
    "        self.weight = torch.randn(chan_in,chan_out)/(chan_in)*0.5\n",
    "        self.bias = torch.zeros(chan_out) if bias else None\n",
    "    def __call__(self,x):\n",
    "        assert x.ndim >=2, \"Input tensor must have at least 2 dimensions\"\n",
    "        linear = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            linear += self.bias\n",
    "        self.out = torch.tanh(linear) * (1/1 + torch.exp(-(self.weight)))\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def __call__(self,x):\n",
    "        x = torch.max(x, dim=-1, keepdim=True)\n",
    "        z = torch.exp(x)\n",
    "        sum_z = z.sum()\n",
    "        self.out = z/sum_z\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42); #set rng for reproductivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76579\n"
     ]
    }
   ],
   "source": [
    "n_emb = 24 #dimensionality of the character embedding vectors\n",
    "n_hidden = 128 #Number of neurons in the hidden layer\n",
    "# C = torch.randn((vocab_size,n_emb))\n",
    "# model = Sequential([\n",
    "#   Embedding(vocab_size, n_emb),\n",
    "#   FlattenConsecutive(2), Linear(n_emb * 2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "#   FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "#   FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "#   Linear(n_hidden, vocab_size),\n",
    "# ])\n",
    "\n",
    "model = Sequential([\n",
    "   Embedding(vocab_size, n_emb),\n",
    "   FlattenConsecutive(2), GLU(n_emb * 2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "   FlattenConsecutive(2), GLU(n_hidden*2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "   FlattenConsecutive(2), GLU(n_hidden*2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "   GLU(n_hidden, vocab_size),\n",
    " ])\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  model.layers[-1].weight *= 0.1\n",
    "\n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2980\n",
      "   1000/ 200000: 2.1462\n",
      "   2000/ 200000: 2.1083\n",
      "   3000/ 200000: 2.3964\n",
      "   4000/ 200000: 2.2212\n",
      "   5000/ 200000: 1.9426\n",
      "   6000/ 200000: 2.0483\n",
      "   7000/ 200000: 1.9543\n",
      "   8000/ 200000: 2.2971\n",
      "   9000/ 200000: 2.3821\n",
      "  10000/ 200000: 2.2351\n",
      "  11000/ 200000: 2.4654\n",
      "  12000/ 200000: 1.9040\n",
      "  13000/ 200000: 2.1101\n",
      "  14000/ 200000: 2.1189\n",
      "  15000/ 200000: 2.5206\n",
      "  16000/ 200000: 2.2419\n",
      "  17000/ 200000: 2.4427\n",
      "  18000/ 200000: 2.2807\n",
      "  19000/ 200000: 1.9829\n",
      "  20000/ 200000: 2.0883\n",
      "  21000/ 200000: 2.2476\n",
      "  22000/ 200000: 1.9121\n",
      "  23000/ 200000: 2.0251\n",
      "  24000/ 200000: 2.5959\n",
      "  25000/ 200000: 1.9664\n",
      "  26000/ 200000: 2.0941\n",
      "  27000/ 200000: 1.9138\n",
      "  28000/ 200000: 2.3344\n",
      "  29000/ 200000: 2.1372\n",
      "  30000/ 200000: 1.7889\n",
      "  31000/ 200000: 2.1765\n",
      "  32000/ 200000: 1.7451\n",
      "  33000/ 200000: 1.7886\n",
      "  34000/ 200000: 2.2570\n",
      "  35000/ 200000: 1.8566\n",
      "  36000/ 200000: 1.7493\n",
      "  37000/ 200000: 1.8680\n",
      "  38000/ 200000: 1.8875\n",
      "  39000/ 200000: 2.4067\n",
      "  40000/ 200000: 2.0336\n",
      "  41000/ 200000: 2.0405\n",
      "  42000/ 200000: 2.2506\n",
      "  43000/ 200000: 1.7914\n",
      "  44000/ 200000: 1.9134\n",
      "  45000/ 200000: 2.0710\n",
      "  46000/ 200000: 2.1507\n",
      "  47000/ 200000: 1.7549\n",
      "  48000/ 200000: 1.9353\n",
      "  49000/ 200000: 2.1148\n",
      "  50000/ 200000: 1.9580\n",
      "  51000/ 200000: 2.1109\n",
      "  52000/ 200000: 1.8567\n",
      "  53000/ 200000: 1.8725\n",
      "  54000/ 200000: 2.1328\n",
      "  55000/ 200000: 1.6859\n",
      "  56000/ 200000: 1.5459\n",
      "  57000/ 200000: 1.7175\n",
      "  58000/ 200000: 1.6074\n",
      "  59000/ 200000: 1.7574\n",
      "  60000/ 200000: 2.1586\n",
      "  61000/ 200000: 1.7507\n",
      "  62000/ 200000: 2.0618\n",
      "  63000/ 200000: 1.8154\n",
      "  64000/ 200000: 2.0809\n",
      "  65000/ 200000: 1.8219\n",
      "  66000/ 200000: 1.9012\n",
      "  67000/ 200000: 1.7738\n",
      "  68000/ 200000: 1.6571\n",
      "  69000/ 200000: 2.0304\n",
      "  70000/ 200000: 2.4309\n",
      "  71000/ 200000: 2.0051\n",
      "  72000/ 200000: 1.9443\n",
      "  73000/ 200000: 1.7796\n",
      "  74000/ 200000: 1.8166\n",
      "  75000/ 200000: 2.2291\n",
      "  76000/ 200000: 2.0000\n",
      "  77000/ 200000: 2.0085\n",
      "  78000/ 200000: 2.2670\n",
      "  79000/ 200000: 2.1272\n",
      "  80000/ 200000: 2.1406\n",
      "  81000/ 200000: 1.6808\n",
      "  82000/ 200000: 2.1043\n",
      "  83000/ 200000: 1.4687\n",
      "  84000/ 200000: 1.3763\n",
      "  85000/ 200000: 2.0014\n",
      "  86000/ 200000: 1.9250\n",
      "  87000/ 200000: 2.1765\n",
      "  88000/ 200000: 1.8902\n",
      "  89000/ 200000: 1.9137\n",
      "  90000/ 200000: 2.0472\n",
      "  91000/ 200000: 1.8308\n",
      "  92000/ 200000: 1.9281\n",
      "  93000/ 200000: 2.0365\n",
      "  94000/ 200000: 1.7863\n",
      "  95000/ 200000: 2.4391\n",
      "  96000/ 200000: 2.2796\n",
      "  97000/ 200000: 2.7137\n",
      "  98000/ 200000: 2.0406\n",
      "  99000/ 200000: 2.2721\n",
      " 100000/ 200000: 2.0164\n",
      " 101000/ 200000: 1.5965\n",
      " 102000/ 200000: 2.0145\n",
      " 103000/ 200000: 1.9348\n",
      " 104000/ 200000: 2.6527\n",
      " 105000/ 200000: 1.8607\n",
      " 106000/ 200000: 1.6735\n",
      " 107000/ 200000: 2.0221\n",
      " 108000/ 200000: 1.9425\n",
      " 109000/ 200000: 2.0030\n",
      " 110000/ 200000: 1.8354\n",
      " 111000/ 200000: 2.1925\n",
      " 112000/ 200000: 1.6498\n",
      " 113000/ 200000: 1.8366\n",
      " 114000/ 200000: 1.6392\n",
      " 115000/ 200000: 1.9001\n",
      " 116000/ 200000: 2.4284\n",
      " 117000/ 200000: 1.6821\n",
      " 118000/ 200000: 2.1564\n",
      " 119000/ 200000: 1.8769\n",
      " 120000/ 200000: 1.8728\n",
      " 121000/ 200000: 2.0039\n",
      " 122000/ 200000: 1.6530\n",
      " 123000/ 200000: 2.0694\n",
      " 124000/ 200000: 1.7234\n",
      " 125000/ 200000: 1.9182\n",
      " 126000/ 200000: 2.1080\n",
      " 127000/ 200000: 2.0948\n",
      " 128000/ 200000: 2.0336\n",
      " 129000/ 200000: 2.3894\n",
      " 130000/ 200000: 1.8624\n",
      " 131000/ 200000: 2.2277\n",
      " 132000/ 200000: 2.1002\n",
      " 133000/ 200000: 1.7219\n",
      " 134000/ 200000: 1.9466\n",
      " 135000/ 200000: 1.9463\n",
      " 136000/ 200000: 2.1056\n",
      " 137000/ 200000: 1.8721\n",
      " 138000/ 200000: 1.7694\n",
      " 139000/ 200000: 1.8730\n",
      " 140000/ 200000: 1.9256\n",
      " 141000/ 200000: 1.9278\n",
      " 142000/ 200000: 1.7647\n",
      " 143000/ 200000: 2.0964\n",
      " 144000/ 200000: 2.0043\n",
      " 145000/ 200000: 1.4540\n",
      " 146000/ 200000: 1.9420\n",
      " 147000/ 200000: 2.5159\n",
      " 148000/ 200000: 2.1422\n",
      " 149000/ 200000: 1.8992\n",
      " 150000/ 200000: 1.7994\n",
      " 151000/ 200000: 1.7925\n",
      " 152000/ 200000: 1.7532\n",
      " 153000/ 200000: 1.8686\n",
      " 154000/ 200000: 2.1382\n",
      " 155000/ 200000: 1.5422\n",
      " 156000/ 200000: 2.0658\n",
      " 157000/ 200000: 1.9043\n",
      " 158000/ 200000: 2.2473\n",
      " 159000/ 200000: 2.4113\n",
      " 160000/ 200000: 2.0531\n",
      " 161000/ 200000: 2.2317\n",
      " 162000/ 200000: 2.2406\n",
      " 163000/ 200000: 2.0806\n",
      " 164000/ 200000: 1.8185\n",
      " 165000/ 200000: 1.7746\n",
      " 166000/ 200000: 1.9612\n",
      " 167000/ 200000: 1.7132\n",
      " 168000/ 200000: 2.0473\n",
      " 169000/ 200000: 2.0552\n",
      " 170000/ 200000: 2.0137\n",
      " 171000/ 200000: 1.8303\n",
      " 172000/ 200000: 1.9022\n",
      " 173000/ 200000: 1.9879\n",
      " 174000/ 200000: 1.9180\n",
      " 175000/ 200000: 1.8010\n",
      " 176000/ 200000: 1.7768\n",
      " 177000/ 200000: 1.8596\n",
      " 178000/ 200000: 1.7826\n",
      " 179000/ 200000: 1.9022\n",
      " 180000/ 200000: 1.9665\n",
      " 181000/ 200000: 1.9736\n",
      " 182000/ 200000: 1.8498\n",
      " 183000/ 200000: 1.8808\n",
      " 184000/ 200000: 2.0073\n",
      " 185000/ 200000: 1.9664\n",
      " 186000/ 200000: 1.4451\n",
      " 187000/ 200000: 2.0980\n",
      " 188000/ 200000: 1.8441\n",
      " 189000/ 200000: 1.7247\n",
      " 190000/ 200000: 1.8560\n",
      " 191000/ 200000: 1.6931\n",
      " 192000/ 200000: 1.3244\n",
      " 193000/ 200000: 1.8356\n",
      " 194000/ 200000: 1.7949\n",
      " 195000/ 200000: 1.7549\n",
      " 196000/ 200000: 2.0806\n",
      " 197000/ 200000: 1.7618\n",
      " 198000/ 200000: 2.0357\n",
      " 199000/ 200000: 1.5205\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    #mini batch\n",
    "    ix = torch.randint(0,Xtr.shape[0],(batch_size,)) #batch size\n",
    "    Xb,Yb = Xtr[ix],Ytr[ix]\n",
    "    #forward pass\n",
    "    # emb = C[Xb] #embed the characters into vectors\n",
    "    # x = emb.view(emb.shape[0],-1) #concatenate the vectors\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits,Yb)\n",
    "    #back pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    # lr = lrs[i]\n",
    "    lr = 0.1 if 1 < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    #track stats\n",
    "    if i % 1000 == 0:\n",
    "        print(f'{i:7d}/{max_steps: 7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layers in eval mode\n",
    "for layer in model.layers:\n",
    "  layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1.9798122644424438\n",
      "val 2.067711591720581\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  logits = model(x)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lansan.\n",
      "dyluna.\n",
      "janella.\n",
      "kashfer.\n",
      "tollersa.\n",
      "eleck.\n",
      "lantri.\n",
      "naiman.\n",
      "rick.\n",
      "melamii.\n",
      "say.\n",
      "syaraquaz.\n",
      "romek.\n",
      "emmerie.\n",
      "ilynn.\n",
      "andruelan.\n",
      "juli.\n",
      "kyamlew.\n",
      "colin.\n",
      "rubik.\n"
     ]
    }
   ],
   "source": [
    "#model test\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass\n",
    "      logits = model(torch.tensor([context]))\n",
    "      # sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
